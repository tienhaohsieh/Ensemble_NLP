{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:33:21.665555Z","iopub.execute_input":"2023-05-11T21:33:21.665884Z","iopub.status.idle":"2023-05-11T21:33:26.454287Z","shell.execute_reply.started":"2023-05-11T21:33:21.665804Z","shell.execute_reply":"2023-05-11T21:33:26.453272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 0. Module installation\n### ref to https://www.kaggle.com/code/leonidkulyk/lb-0-45836-blip-clip-clip-interrogator","metadata":{}},{"cell_type":"code","source":"#!pip install --no-index /kaggle/input/transformer-4180/transformers_4.18.0/sacremoses-0.0.53-py3-none-any.whl\n#!pip install --no-index -q /kaggle/input/transformer-4180/transformers_4.18.0/transformers-4.18.0.dev0-py3-none-any.whl\n# Using the pre compiled wheel since we don't have internet on submission\n!pip install -q /kaggle/input/stable-diffusion-data/transformers-4.18.0.dev0-py3-none-any.whl\n\nfrom transformers import OFATokenizer, OFAModel\nfrom transformers.models.ofa.generate import sequence_generator","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:33:26.455968Z","iopub.execute_input":"2023-05-11T21:33:26.456647Z","iopub.status.idle":"2023-05-11T21:34:06.709198Z","shell.execute_reply.started":"2023-05-11T21:33:26.456608Z","shell.execute_reply":"2023-05-11T21:34:06.708164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nfrom PIL import Image\nfrom pathlib import Path\nimport matplotlib.pyplot as plt \nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.dataloader import default_collate\n\nfrom torchvision import transforms\nfrom sklearn.preprocessing import normalize\n\nimport numpy as np\nimport pandas as pd\nimport torch\n\n!pip install --no-index --find-links /kaggle/input/clip-interrogator-wheels-x /kaggle/input/clip-interrogator-wheels-x/clip_interrogator-0.4.3-py3-none-any.whl -q\nimport inspect\nimport importlib\n\nfrom blip.models import blip\nimport open_clip\nfrom clip_interrogator import clip_interrogator\n\nsys.path.append('/kaggle/input/sentence-transformers-222/sentence-transformers')\nfrom sentence_transformers import SentenceTransformer, models, util","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:34:06.710864Z","iopub.execute_input":"2023-05-11T21:34:06.71154Z","iopub.status.idle":"2023-05-11T21:34:37.81088Z","shell.execute_reply.started":"2023-05-11T21:34:06.711501Z","shell.execute_reply":"2023-05-11T21:34:37.809669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fix clip_interrogator bug\nclip_interrogator_path = inspect.getfile(clip_interrogator.Interrogator)\n\nfin = open(clip_interrogator_path, \"rt\")\ndata = fin.read()\ndata = data.replace(\n    'open_clip.get_tokenizer(clip_model_name)', \n    'open_clip.get_tokenizer(config.clip_model_name.split(\"/\", 2)[0])'\n)\nfin.close()\n\nfin = open(clip_interrogator_path, \"wt\")\nfin.write(data)\nfin.close()\n\n# reload module\nimportlib.reload(clip_interrogator)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:34:37.814305Z","iopub.execute_input":"2023-05-11T21:34:37.815339Z","iopub.status.idle":"2023-05-11T21:34:37.841992Z","shell.execute_reply.started":"2023-05-11T21:34:37.815291Z","shell.execute_reply":"2023-05-11T21:34:37.840768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# 1. Build models\n### standard model\n### Chose all-MiniLM-L6-v2\n### This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.","metadata":{}},{"cell_type":"markdown","source":"## 1.0 Preparation","metadata":{}},{"cell_type":"code","source":"#model = SentenceTransformer('all-MiniLM-L6-v2')\nst_model = SentenceTransformer(\"/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2\")","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:34:37.846013Z","iopub.execute_input":"2023-05-11T21:34:37.846344Z","iopub.status.idle":"2023-05-11T21:34:39.309515Z","shell.execute_reply.started":"2023-05-11T21:34:37.846313Z","shell.execute_reply":"2023-05-11T21:34:39.308495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare functions of encode, scoring, plotting","metadata":{}},{"cell_type":"code","source":"import matplotlib.image as mpimg\nimport textwrap\n\ndef Prompt2Emb(input_li):\n    if isinstance(input_li,str):\n        input_li = [input_li]\n    out_li = []\n    for element in input_li:\n        if type(element) is str:\n            out = st_model.encode(element, show_progress_bar=False)\n        else:\n            out = element\n        if isinstance(out, torch.Tensor):\n            out = out.detach().cpu().numpy()\n        out_li.append(out)\n    output_li = out_li[0] if isinstance(input_li,str) else out_li\n    return output_li\n\ndef scoring(prompt_li, prompt_clip_li):\n    score_li=[]\n    prompt_emb_li = Prompt2Emb(prompt_li)\n    pre_prompt_emb_li = Prompt2Emb(prompt_clip_li)\n    for prompt_emb, predict_emb in zip(prompt_emb_li, pre_prompt_emb_li):\n        cosine_similarity_score = util.cos_sim(prompt_emb.astype(float), predict_emb.astype(float)).numpy().flatten()\n        score_li.append(cosine_similarity_score[0])\n    return score_li\n\ndef image_grid(imgs, prompts='', prompts2='', score_li='', rows=1, cols=7):\n    assert len(imgs) == rows*cols\n    Figure=plt.figure(figsize=(18,15))\n    for idx,img in enumerate(imgs):\n        ax=Figure.add_subplot(cols,rows,idx+1)\n        ax.imshow(img)\n        ax.tick_params(bottom=False,left=False,labelbottom=False,labelleft=False)\n        if score_li!='':\n            ax.annotate('score: %.3f'%(score_li[idx]),xycoords='axes fraction',ha='left',va='top',xy=(1.1,1.0),wrap=True,fontsize=12,c='r')\n        if prompts!='':\n            prompt = prompts[idx]\n            text= '\\n'.join(textwrap.wrap(prompt,60))\n            ax.annotate(text,xycoords='axes fraction',ha='left',va='top',xy=(1.1,0.9),wrap=True,fontsize=12,c='k')\n        if prompts2!='':\n            prompt2 = prompts2[idx]\n            text2= '\\n'.join(textwrap.wrap(prompt2,60))\n            ax.annotate(text2,xycoords='axes fraction',ha='left',va='top',xy=(1.1,0.3),wrap=True,fontsize=12,c='b')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:34:39.313252Z","iopub.execute_input":"2023-05-11T21:34:39.313555Z","iopub.status.idle":"2023-05-11T21:34:39.338557Z","shell.execute_reply.started":"2023-05-11T21:34:39.313527Z","shell.execute_reply":"2023-05-11T21:34:39.337434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = True","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:34:39.340379Z","iopub.execute_input":"2023-05-11T21:34:39.340934Z","iopub.status.idle":"2023-05-11T21:34:39.348675Z","shell.execute_reply.started":"2023-05-11T21:34:39.340865Z","shell.execute_reply":"2023-05-11T21:34:39.34773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.1 CLIP interrogator model","metadata":{}},{"cell_type":"markdown","source":"### 1.1.1 build model","metadata":{}},{"cell_type":"code","source":"class CFG:\n    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n#    seed = 42\n    embedding_length = 384\n#    sentence_model_path = \"/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2\"\n    blip_model_path = \"/kaggle/input/clip-interrogator-models-x/model_large_caption.pth\"\n    ci_clip_model_name = \"ViT-H-14/laion2b_s32b_b79k\"\n    clip_model_name = \"ViT-H-14\"\n    clip_model_path = \"/kaggle/input/clip-interrogator-models-x/CLIP-ViT-H-14-laion2B-s32B-b79K/open_clip_pytorch_model.bin\"\n    cache_path = \"/kaggle/input/clip-interrogator-models-x\"","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:34:39.34992Z","iopub.execute_input":"2023-05-11T21:34:39.350799Z","iopub.status.idle":"2023-05-11T21:34:39.358059Z","shell.execute_reply.started":"2023-05-11T21:34:39.350757Z","shell.execute_reply":"2023-05-11T21:34:39.357032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CLIP Interrogator uses OpenCLIP which supports many different pretrained CLIP models.\n### For the best prompts for Stable Diffusion 1.X use ViT-L-14/openai for clip_model_name.\n### For Stable Diffusion 2.0 use ViT-H-14/laion2b_s32b_b79k","metadata":{}},{"cell_type":"code","source":"# replace tokenizer path to prevent downloading\nblip_path = inspect.getfile(blip)\n\nfin = open(blip_path, \"rt\")\ndata = fin.read()\ndata = data.replace(\n    \"BertTokenizer.from_pretrained('bert-base-uncased')\", \n    \"BertTokenizer.from_pretrained('/kaggle/input/clip-interrogator-models-x/bert-base-uncased')\"\n)\nfin.close()\n\nfin = open(blip_path, \"wt\")\nfin.write(data)\nfin.close()\n\n# reload module\nimportlib.reload(blip)\ndel data","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:34:39.359879Z","iopub.execute_input":"2023-05-11T21:34:39.360247Z","iopub.status.idle":"2023-05-11T21:34:39.373973Z","shell.execute_reply.started":"2023-05-11T21:34:39.360212Z","shell.execute_reply":"2023-05-11T21:34:39.373041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This part I haven't really understand it","metadata":{}},{"cell_type":"code","source":"model_config = clip_interrogator.Config(clip_model_name = CFG.ci_clip_model_name)\nmodel_config.cache_path = CFG.cache_path\n\n#blip_path = \"/opt/conda/lib/python3.7/site-packages/blip/models/blip.py\"\nconfigs_path = os.path.join(os.path.dirname(os.path.dirname(blip_path)), 'configs')\nmed_config = os.path.join(configs_path, 'med_config.json')\nblip_model = blip.blip_decoder(\n    pretrained=CFG.blip_model_path,\n    image_size=model_config.blip_image_eval_size, \n    vit=model_config.blip_model_type, \n    med_config=med_config\n)\n\nblip_model.eval()\nblip_model = blip_model.to(model_config.device)\nmodel_config.blip_model = blip_model\n","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:34:39.379261Z","iopub.execute_input":"2023-05-11T21:34:39.379545Z","iopub.status.idle":"2023-05-11T21:35:11.467239Z","shell.execute_reply.started":"2023-05-11T21:34:39.379501Z","shell.execute_reply":"2023-05-11T21:35:11.466213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clip_model = open_clip.create_model(CFG.clip_model_name, precision='fp16' if model_config.device == 'cuda' else 'fp32')\nopen_clip.load_checkpoint(clip_model, CFG.clip_model_path)\nclip_model.to(model_config.device).eval()\nmodel_config.clip_model = clip_model\n\nclip_preprocess = open_clip.image_transform(\n    clip_model.visual.image_size,\n    is_train = False,\n    mean = getattr(clip_model.visual, 'image_mean', None),\n    std = getattr(clip_model.visual, 'image_std', None),\n)\n\nmodel_config.clip_preprocess = clip_preprocess\n\nci = clip_interrogator.Interrogator(model_config)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:35:11.468702Z","iopub.execute_input":"2023-05-11T21:35:11.469165Z","iopub.status.idle":"2023-05-11T21:36:01.893658Z","shell.execute_reply.started":"2023-05-11T21:35:11.469119Z","shell.execute_reply":"2023-05-11T21:36:01.892642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the inverse transform\ninverse_transform = transforms.Compose([\n    transforms.Normalize(mean=[0, 0, 0], std=[1/0.229, 1/0.224, 1/0.225]),\n    transforms.Normalize(mean=[-0.485, -0.456, -0.406], std=[1, 1, 1]),\n    transforms.ToPILImage()\n])\n\ncos = torch.nn.CosineSimilarity(dim=1)\n\nmediums_features_array = torch.stack([torch.from_numpy(t) for t in ci.mediums.embeds]).to(ci.device)\nmovements_features_array = torch.stack([torch.from_numpy(t) for t in ci.movements.embeds]).to(ci.device)\nflavors_features_array = torch.stack([torch.from_numpy(t) for t in ci.flavors.embeds]).to(ci.device)\n\nclass MyClip:\n    def __init__(self, model = ci, withFlave = True):\n        self.model = model\n        self.withFlave = withFlave\n    def img_list(self, img_list):\n        if len(img_list) == 1 and not isinstance(img_list, list) and not isinstance(img_list, torch.Tensor):\n            img_list = [img_list]\n        if isinstance(img_list[0], torch.Tensor):\n            img_list = [inverse_transform(img) for img in img_list]\n        else:\n            pass\n        return img_list\n    def predict_prompt(self, img_list):\n        img_list = self.img_list(img_list)\n        prompts = []\n        for img in img_list:\n            prompt = self.model.generate_caption(img)\n            if self.withFlave:\n                medium, movement, flaves = self.feature(img)\n                if prompt.startswith(medium):\n                    prompt = f\"{prompt}, {movement}, {flaves}\"\n                else:\n                    prompt = f\"{prompt}, {medium}, {movement}, {flaves}\"\n                prompt = clip_interrogator._truncate_to_fit(prompt, self.model.tokenize)\n            prompts.append(prompt)\n        return prompts[0] if len(prompts)==1 else prompts\n    def feature(self, img):\n        image_features = self.model.image_to_features(img)\n        medium = [self.model.mediums.labels[i] for i in cos(image_features, mediums_features_array).topk(1).indices][0]\n        movement = [self.model.movements.labels[i] for i in cos(image_features, movements_features_array).topk(1).indices][0]\n        flaves = \", \".join([self.model.flavors.labels[i] for i in cos(image_features, flavors_features_array).topk(3).indices])\n        return medium, movement, flaves\n    \n    def predict_emb(self, img_list):\n        prompts = self.predict_prompt(img_list)\n        embeddings = Prompt2Emb(prompts)\n        return embeddings[0] if len(embeddings)==1 else embeddings\n\nmy_clip = MyClip()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:36:01.895236Z","iopub.execute_input":"2023-05-11T21:36:01.895817Z","iopub.status.idle":"2023-05-11T21:36:02.355558Z","shell.execute_reply.started":"2023-05-11T21:36:01.895777Z","shell.execute_reply":"2023-05-11T21:36:02.354538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.2 Test sample images\n### check scores and imaging for sample","metadata":{}},{"cell_type":"code","source":"my_clip = MyClip(withFlave = True)\nif not submission:\n    sample_prompt_li = pd.read_csv(\"/kaggle/input/stable-diffusion-image-to-prompts/prompts.csv\")\n    sample_img_li = ['/kaggle/input/stable-diffusion-image-to-prompts/images/'+imgId+'.png' for imgId in sample_prompt_li['imgId']]\n    sample_prompt_li = [prompt for prompt in sample_prompt_li['prompt']]\n    sample_img_li = [Image.open(img).convert(\"RGB\") for img in sample_img_li]\n\n    prompt_clip_li = my_clip.predict_prompt(sample_img_li)\n    test_li = st_model.encode(prompt_clip_li)\n\n    #score_li=scoring(sample_prompt_li, prompt_clip_li)\n    score_li=scoring(sample_prompt_li, test_li)\n    print(score_li)\n    print(np.average(score_li))\n\n    image_grid(sample_img_li, sample_prompt_li, prompt_clip_li, score_li, 1, 7)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:36:02.35718Z","iopub.execute_input":"2023-05-11T21:36:02.357611Z","iopub.status.idle":"2023-05-11T21:36:19.748755Z","shell.execute_reply.started":"2023-05-11T21:36:02.357567Z","shell.execute_reply":"2023-05-11T21:36:19.747803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_clip = MyClip(withFlave = True)\nif not submission:\n    sample_prompt_li = pd.read_csv(\"/kaggle/input/stable-diffusion-image-to-prompts/prompts.csv\")\n    sample_img_li = ['/kaggle/input/stable-diffusion-image-to-prompts/images/'+imgId+'.png' for imgId in sample_prompt_li['imgId']]\n    sample_prompt_li = [prompt for prompt in sample_prompt_li['prompt']]\n    sample_img_li = [Image.open(img).convert(\"RGB\") for img in sample_img_li]\n\n    prompt_clip_li = my_clip.predict_prompt(sample_img_li)\n    test_li = st_model.encode(prompt_clip_li)\n\n    #score_li=scoring(sample_prompt_li, prompt_clip_li)\n    score_li=scoring(sample_prompt_li, test_li)\n    print(score_li)\n    print(np.average(score_li))","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:36:19.749833Z","iopub.execute_input":"2023-05-11T21:36:19.750264Z","iopub.status.idle":"2023-05-11T21:36:24.364848Z","shell.execute_reply.started":"2023-05-11T21:36:19.750226Z","shell.execute_reply":"2023-05-11T21:36:24.363753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 ViT-B-16 Model","metadata":{}},{"cell_type":"markdown","source":"### 1.2.1 Build model¶","metadata":{}},{"cell_type":"code","source":"class ViT_CFG:\n    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n    model_path = '/kaggle/input/saved-vit-model/vit_large_patch16_384_1_64_0.0001_0.6564.pth'\n    model_name = 'vit_large_patch16_384'\n    input_size = (384, 384)\n    batch_size = 64","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:36:24.366502Z","iopub.execute_input":"2023-05-11T21:36:24.366891Z","iopub.status.idle":"2023-05-11T21:36:24.374263Z","shell.execute_reply.started":"2023-05-11T21:36:24.366853Z","shell.execute_reply":"2023-05-11T21:36:24.373028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import timm\n\nvit_model = timm.create_model(\n    model_name = ViT_CFG.model_name,\n    pretrained = False,\n    num_classes = 384\n)\nstate_dict = torch.load(ViT_CFG.model_path)\nvit_model.load_state_dict(state_dict)\nvit_model.to(ViT_CFG.device)\nvit_model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:36:24.375541Z","iopub.execute_input":"2023-05-11T21:36:24.376691Z","iopub.status.idle":"2023-05-11T21:36:40.549996Z","shell.execute_reply.started":"2023-05-11T21:36:24.376653Z","shell.execute_reply":"2023-05-11T21:36:40.548894Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\nclass MyViT:\n    def __init__(self, model = vit_model, tta=2):\n        self.model = model\n        self.tta = tta\n        self.transformPIL = transforms.Compose([\n                        transforms.Resize(ViT_CFG.input_size),\n                        transforms.RandomHorizontalFlip(p=0.5),\n                        transforms.ToTensor(),\n                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n                        ])\n        \n    def img_list(self, img_list): # The output img_list need to be torch.tensor\n        if isinstance(img_list, torch.Tensor):\n            if img_list.dim() == 3:\n                img_list = img_list.unsqueeze(0)\n            img_list = F.interpolate(img_list, size=ViT_CFG.input_size, mode='bilinear', align_corners=False).to(ViT_CFG.device)\n        else:\n            if isinstance(img_list, Image.Image):\n                img_list = [img_list]\n            img_list = [self.transformPIL(img).to(ViT_CFG.device) for img in img_list]\n        return img_list\n    def predict_emb(self, img_list):\n        img_list = self.img_list(img_list)\n        embeddings = []\n        for img_emb in img_list:\n            to_stack = np.empty((0, CFG.embedding_length))\n            for t in np.arange(self.tta):\n                embedding = self.model(img_emb.unsqueeze(0)).squeeze(0).detach().cpu().numpy()\n                #embedding = embedding/(abs(embedding).max()+0.0000001)\n                to_stack = np.vstack((to_stack,embedding))\n            embedding = np.average(to_stack, axis=0)\n            embedding = embedding / np.linalg.norm(embedding)\n            embeddings.append(embedding)\n        return embeddings\nmy_vit = MyViT()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:48:43.628255Z","iopub.execute_input":"2023-05-11T21:48:43.62862Z","iopub.status.idle":"2023-05-11T21:48:43.64337Z","shell.execute_reply.started":"2023-05-11T21:48:43.628589Z","shell.execute_reply":"2023-05-11T21:48:43.642204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2.2 Test sample images","metadata":{}},{"cell_type":"code","source":"my_vit = MyViT(tta=2)\nif not submission:\n    predict_emb_li = my_vit.predict_emb(sample_img_li)\n    #predict_emb_li = predict_emb_li /( np.abs(predict_emb_li).max(axis=-1, keepdims=True) + 0.0000001)\n    #predict_emb_li = normalize(predict_emb_li) # I am not sure why\n    score_li = scoring(sample_prompt_li, predict_emb_li)\n    print(score_li)\n    print(np.average(score_li))","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:48:46.653505Z","iopub.execute_input":"2023-05-11T21:48:46.653878Z","iopub.status.idle":"2023-05-11T21:48:47.75435Z","shell.execute_reply.started":"2023-05-11T21:48:46.653846Z","shell.execute_reply":"2023-05-11T21:48:47.753192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 OFA model","metadata":{}},{"cell_type":"markdown","source":"### https://www.kaggle.com/code/parikshitsharma2001/lb-0-55161-clipinterrogator-ofa-vit","metadata":{}},{"cell_type":"markdown","source":"### 1.3.1 Build Model","metadata":{}},{"cell_type":"code","source":"class OFA_CFG:\n    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n    #model_path = '/kaggle/input/stable-diffusion-vit-baseline-train/vit_base_patch16_224.pth'\n    #model_path = '/kaggle/input/vit-large/vit_large_patch16_224.pth'\n    #model_name = 'vit_large_patch16_224'\n    #model_path = '/kaggle/input/timm-vit-huge-patch14-224-in21k/pytorch_model.bin'\n    #model_name = 'vit_huge_patch14_224_in21k'\n    #input_size = (224, 224)\n    #batch_size = 64","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:43:36.01588Z","iopub.execute_input":"2023-05-11T21:43:36.016743Z","iopub.status.idle":"2023-05-11T21:43:36.022305Z","shell.execute_reply.started":"2023-05-11T21:43:36.016699Z","shell.execute_reply":"2023-05-11T21:43:36.021002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CKPT_DIR = \"/kaggle/input/stable-diffusion-data/OFA-large-caption/\"\n\nofa_tokenizer = OFATokenizer.from_pretrained(CKPT_DIR)\nofa_model = OFAModel.from_pretrained(CKPT_DIR, use_cache=False).cuda()\ntxt = \" what does the image describe?\"\nask_inputs = ofa_tokenizer([txt], return_tensors=\"pt\").input_ids","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:43:39.173258Z","iopub.execute_input":"2023-05-11T21:43:39.173636Z","iopub.status.idle":"2023-05-11T21:44:06.735291Z","shell.execute_reply.started":"2023-05-11T21:43:39.173604Z","shell.execute_reply":"2023-05-11T21:44:06.734255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean, std = [1, 1, 1], [1, 1, 1]\n#mean, std = [0.0, 0.0, 0.0], [1.0, 1.0, 1.0]\nresolution = 512 # I change from 480 (default) to the original resolution, or we just remove resize\nofa_transform = transforms.Compose([\n        lambda image: image.convert(\"RGB\"),\n        #transforms.Resize((resolution, resolution), interpolation=Image.BICUBIC),\n        transforms.ToTensor(), \n        transforms.Normalize(mean=mean, std=std)\n#        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ])","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:44:39.824064Z","iopub.execute_input":"2023-05-11T21:44:39.824458Z","iopub.status.idle":"2023-05-11T21:44:39.83127Z","shell.execute_reply.started":"2023-05-11T21:44:39.824423Z","shell.execute_reply":"2023-05-11T21:44:39.830112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyOFA:\n    def __init__(self, model = ofa_model):\n        self.model = model\n    def img_list(self, img_list):\n        if len(img_list) == 1 and not isinstance(img_list, list) and not isinstance(img_list, torch.Tensor):\n            img_list = [img_list]\n        if isinstance(img_list[0], Image.Image):\n            img_list = [ofa_transform(img).unsqueeze(0).to(OFA_CFG.device) for img in img_list]\n        elif isinstance(img_list[0], torch.Tensor):\n            img_list = [img.unsqueeze(0) for img in img_list]\n        else:\n            pass\n        return img_list\n        \n    def predict_prompt(self, img_list):\n        img_list = self.img_list(img_list)\n        prompts = []\n        for img in img_list:\n            emb_ofa = self.model.generate(ask_inputs.cuda(), patch_images=img, num_beams=5, no_repeat_ngram_size=2)\n            prompt = ofa_tokenizer.batch_decode(emb_ofa, skip_special_tokens=True)[0]\n            prompts.append(prompt)\n        return prompts[0] if len(prompts)==1 else prompts\n\n    def predict_emb(self, img_list):\n        prompts = self.predict_prompt(img_list)\n        embeddings = Prompt2Emb(prompts)\n        embeddings = embeddings / np.linalg.norm(embeddings)\n        return embeddings[0] if len(embeddings)==1 else embeddings\n\nmy_ofa = MyOFA()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:44:42.7611Z","iopub.execute_input":"2023-05-11T21:44:42.761474Z","iopub.status.idle":"2023-05-11T21:44:42.775221Z","shell.execute_reply.started":"2023-05-11T21:44:42.761443Z","shell.execute_reply":"2023-05-11T21:44:42.774163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from gensim.parsing.preprocessing import remove_stopwords\nmy_ofa = MyOFA()\nif not submission:\n    sample_prompt_li = pd.read_csv(\"/kaggle/input/stable-diffusion-image-to-prompts/prompts.csv\")\n    sample_img_li = ['/kaggle/input/stable-diffusion-image-to-prompts/images/'+imgId+'.png' for imgId in sample_prompt_li['imgId']]\n    sample_prompt_li = [prompt for prompt in sample_prompt_li['prompt']]\n    sample_img_li = [Image.open(img).convert(\"RGB\") for img in sample_img_li]\n\n    prompt_ofa_li = my_ofa.predict_prompt(sample_img_li)\n    #print(prompt_ofa_li)\n    embedding_ofa_li = my_ofa.predict_emb(sample_img_li)\n    \n    score_li=scoring(sample_prompt_li, embedding_ofa_li)\n    print(score_li)\n    print(np.average(score_li))\n\n    image_grid(sample_img_li, sample_prompt_li, prompt_ofa_li, score_li, 1, 7)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:44:46.536456Z","iopub.execute_input":"2023-05-11T21:44:46.536927Z","iopub.status.idle":"2023-05-11T21:45:02.0739Z","shell.execute_reply.started":"2023-05-11T21:44:46.536887Z","shell.execute_reply":"2023-05-11T21:45:02.073016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Online Data (need internet)","metadata":{}},{"cell_type":"markdown","source":"### ref to https://www.kaggle.com/code/debarshichanda/pytorch-blip-training/notebook","metadata":{}},{"cell_type":"markdown","source":"## 3.1 download dataset","metadata":{}},{"cell_type":"markdown","source":"### https://github.com/poloclub/diffusiondb/blob/main/notebooks/example-loading.ipynb","metadata":{}},{"cell_type":"markdown","source":"### check data randomly","metadata":{}},{"cell_type":"markdown","source":"# 4. Ensembling Model","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Ensembling","metadata":{}},{"cell_type":"code","source":"from torch import nn, optim\n​\nInit = torch.tensor([[0.52], [0.3], [0.18]]).repeat(1, 384)\n​\nclass Ensemble_Model(nn.Module):\n    def __init__(self, model_li):\n        super().__init__()\n        self.model1 = model_li[0]\n        self.model2 = model_li[1]\n        self.model3 = model_li[2]\n        #        self.weights = torch.nn.Parameter(torch.tensor([0.52, 0.3, 0.18],dtype=torch.float32, requires_grad=True))\n        self.weights = torch.nn.Parameter(torch.tensor(Init, dtype=torch.float32, requires_grad=True))\n#        # Set weights to sum up to 1\n    def forward(self, x):\n        self.weights /= self.weights.sum(dim=0)\n        # Calculate the embeddings for each model and compute the weighted sum\n        embeddings = torch.zeros((x.size(0), self.model1.embedding_size))\n        for i, model in enumerate([self.model1, self.model2, self.model3]):\n            model_embedding = model.predict_emb(x)\n            embeddings += model_embedding * self.weights[i]\n#        embeddings /= torch.sum(self.weights)\n        return embeddings\n\n    def predict(self, X):\n        embedding1 = self.model1.predict_emb(X)\n        embedding2 = self.model2.predict_emb(X)\n        embedding3 = self.model3.predict_emb(X)\n        embeddings = torch.tensor(embedding1) * self.weights[0]\n        + torch.tensor(embedding2) * self.weights[1]\n        + torch.tensor(embedding3) * self.weights[2]\n        return embeddings\n    \n    def parameters(self):\n        return [self.weights]","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:45:15.037981Z","iopub.execute_input":"2023-05-11T21:45:15.038362Z","iopub.status.idle":"2023-05-11T21:45:15.048194Z","shell.execute_reply.started":"2023-05-11T21:45:15.03833Z","shell.execute_reply":"2023-05-11T21:45:15.046853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Test ensembling model with sample images","metadata":{}},{"cell_type":"code","source":"if not submission:\n    model_li = [my_vit, my_clip, my_ofa]\n    vit_weight_li = [0.6,0.7,0.8]\n    for vit_weight in vit_weight_li:\n        print('ViT weight is:', '%.3f'%vit_weight)\n        weight_li = np.arange(0, 1-vit_weight+0.001, 0.05)\n        for weight in weight_li:\n            weights = [vit_weight, weight, 1-vit_weight-weight]\n            MyModel = Ensemble_Model(model_li, weights = weights)\n            embedding_li = MyModel.predict(sample_img_li)\n            score_li = scoring(sample_prompt_li,embedding_li)\n            print(\"clip w:\",'%.3f'%weight, 'avg_score:','%.3f'%(np.average(score_li)),['%.3f'%score for score in score_li])","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:50:02.127028Z","iopub.execute_input":"2023-05-11T21:50:02.127401Z","iopub.status.idle":"2023-05-11T21:50:02.162816Z","shell.execute_reply.started":"2023-05-11T21:50:02.127367Z","shell.execute_reply":"2023-05-11T21:50:02.161347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Submission¶","metadata":{}},{"cell_type":"markdown","source":"### This is a simpling decode the generated prompts to the vector with 384 dimensional, so 7x384 = 2688","metadata":{}},{"cell_type":"code","source":"# 定義圖像資料集類別\nclass ImageDataset(Dataset):\n    def __init__(self, comp_path, image_names):\n        self.comp_path = comp_path\n        self.image_names = image_names\n        \n        self.transform = transforms.Compose([\n#            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ])\n\n    def __len__(self):\n        return len(self.image_names)\n\n    def __getitem__(self, index):\n        # 讀取圖像並將其轉換為 RGB 格式\n        image_name = self.image_names[index]\n        image_path = os.path.join(self.comp_path, 'images', image_name)\n        image = Image.open(image_path).convert(\"RGB\")\n        image = self.transform(image)#.unsqueeze(0)#.to(ViT_CFG.device)\n        return image","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:45:35.376582Z","iopub.execute_input":"2023-05-11T21:45:35.376976Z","iopub.status.idle":"2023-05-11T21:45:35.385988Z","shell.execute_reply.started":"2023-05-11T21:45:35.376926Z","shell.execute_reply":"2023-05-11T21:45:35.384742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comp_path = Path('/kaggle/input/stable-diffusion-image-to-prompts/')\nimage_names = os.listdir(comp_path / 'images')\nimgIds = [i.split('.')[0] for i in image_names]\n\neIds = list(range(CFG.embedding_length))\n\nimgId_eId = [\n    '_'.join(map(str, i)) for i in zip(\n        np.repeat(imgIds, CFG.embedding_length),\n        np.tile(range(CFG.embedding_length), len(imgIds))\n    )\n]\n\nimg_li = ImageDataset(comp_path, image_names)  # 創建圖像資料集對象\n# 創建 DataLoader 對象\ndataloader = DataLoader(dataset=img_li,  # 使用上面創建的圖像資料集對象\n                        batch_size=2,  # 每批次讀取的圖像數量\n#                        num_workers=1,  # 用於讀取數據的工作進程數\n                        shuffle=False)  # 是否打亂資料集順序，此處不打亂","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:45:38.811735Z","iopub.execute_input":"2023-05-11T21:45:38.812133Z","iopub.status.idle":"2023-05-11T21:45:38.82978Z","shell.execute_reply.started":"2023-05-11T21:45:38.812099Z","shell.execute_reply":"2023-05-11T21:45:38.828663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission = pd.read_csv(comp_path / 'sample_submission.csv', index_col='imgId_eId')\nassert sorted(imgId_eId) == sorted(df_submission.index)\n\nmy_model = Ensemble_Model([my_vit, my_clip, my_ofa])\ndata_path = '/kaggle/input/ensembleweight-hsieh/'\nif os.path.exists(data_path+'ensemble_model_weights.pth'):\n    my_model.load_state_dict(torch.load(data_path+'ensemble_model_weights.pth'))\n    accuracy_li = np.loadtxt(data_path+'/accuracy.txt')\n    print('Using from last training point')\nelse:\n    accuracy_li = np.array([])\n\npredict_emb_li = []\nfor _, inputs in enumerate(tqdm(dataloader, total=len(dataloader))):\n    inputs = inputs.to(CFG.device)\n    \n    prompt = MyModel.predict(inputs)\n    predict_emb_li.extend(prompt)\n\npredict_emb_li = np.array(predict_emb_li).flatten()\n\nprint('Size',np.shape(predict_emb_li))\nsubmission = pd.DataFrame(\n    index=imgId_eId,\n    data=predict_emb_li,\n    columns=['val']\n).rename_axis('imgId_eId')\n\n#display(df_submission)\n#score_li = scoring(np.array(df_submission.sort_index()['val']).reshape(7,384),np.array(submission.sort_index()['val']).reshape(7, 384))\n#print(score_li, np.average(score_li))\n#display(submission)\n\nsubmission.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:49:22.577036Z","iopub.execute_input":"2023-05-11T21:49:22.577405Z","iopub.status.idle":"2023-05-11T21:49:34.737896Z","shell.execute_reply.started":"2023-05-11T21:49:22.577374Z","shell.execute_reply":"2023-05-11T21:49:34.736653Z"},"trusted":true},"execution_count":null,"outputs":[]}]}